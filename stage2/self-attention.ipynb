{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e179bcb",
   "metadata": {},
   "source": [
    "# Self-Attention with trainable weights\n",
    "\n",
    "Each input embedded token is mapped to a key and value \n",
    "Each key is used once as a query, to calculate the dot product like before "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845966a",
   "metadata": {},
   "source": [
    "## need to do this for all input tokens\n",
    "\n",
    "x = [[1,2,3],[4,5,6],...] shape = data_size x no_embedding_dims\n",
    "\n",
    "X[1] = [1, 2, 3] shape = 1 x no_embedding_dims \n",
    "\n",
    "3 weight layers map X[1] to Wq, Wk and Wv\n",
    "Wk = Wkey shape = no_of_embedding_dims x output_dims (eg. 2)\n",
    "Wv = Wvalue shape = ^ \n",
    "Wq = Query shape = ^\n",
    "\n",
    "k = key\n",
    "k[1] = X[1] * Wk shape = data_size x output_dims\n",
    "v = value\n",
    "v[1] = X[1] * Wv shape = ^\n",
    "q[1] = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65075c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46c1765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = inputs[1]\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a31dbf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1366, 0.1025],\n",
       "        [0.1841, 0.7264],\n",
       "        [0.3153, 0.6871]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "weight_query = torch.nn.Parameter(torch.rand(x2.shape[0], 2), requires_grad=False)\n",
    "weight_key = torch.nn.Parameter(torch.rand(x2.shape[0], 2), requires_grad=False)\n",
    "weight_value = torch.nn.Parameter(torch.rand(x2.shape[0], 2), requires_grad=False)\n",
    "\n",
    "weight_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bc30c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = x2 @ weight_query\n",
    "key2 = x2 @ weight_key\n",
    "value2 = x2 @ weight_value\n",
    "\n",
    "query2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb0263",
   "metadata": {},
   "source": [
    "for all values do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d0b250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ weight_key\n",
    "values = inputs @ weight_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb827221",
   "metadata": {},
   "source": [
    "## attention score \n",
    "\n",
    "attention_score_22 = dot product between the query vector of the token 2 and key of the iter-ed token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "812361e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_22 = query2.dot(keys[1])\n",
    "attention_scores_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f009cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_2 = query2 @ keys.T\n",
    "attention_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa45f91",
   "metadata": {},
   "source": [
    "weighted sum with attention weight and value builds context vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3754c74",
   "metadata": {},
   "source": [
    "# SelfAttentionV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b376eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vectors = attention_weights @ values\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81f93cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "selfAttention = SelfAttentionV1(3, 2)\n",
    "selfAttention(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48c683",
   "metadata": {},
   "source": [
    "# SelfAttentionV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20f59f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vectors = attention_weights @ values \n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5011c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "selfAttention = SelfAttentionV2(3, 2)\n",
    "selfAttention(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3268165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
